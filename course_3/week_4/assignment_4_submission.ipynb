{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.0** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-machine-learning/resources/bANLa) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Assignment 4 - Understanding and Predicting Property Maintenance Fines\n",
    "\n",
    "This assignment is based on a data challenge from the Michigan Data Science Team ([MDST](http://midas.umich.edu/mdst/)). \n",
    "\n",
    "The Michigan Data Science Team ([MDST](http://midas.umich.edu/mdst/)) and the Michigan Student Symposium for Interdisciplinary Statistical Sciences ([MSSISS](https://sites.lsa.umich.edu/mssiss/)) have partnered with the City of Detroit to help solve one of the most pressing problems facing Detroit - blight. [Blight violations](http://www.detroitmi.gov/How-Do-I/Report/Blight-Complaint-FAQs) are issued by the city to individuals who allow their properties to remain in a deteriorated condition. Every year, the city of Detroit issues millions of dollars in fines to residents and every year, many of these fines remain unpaid. Enforcing unpaid blight fines is a costly and tedious process, so the city wants to know: how can we increase blight ticket compliance?\n",
    "\n",
    "The first step in answering this question is understanding when and why a resident might fail to comply with a blight ticket. This is where predictive modeling comes in. For this assignment, your task is to predict whether a given blight ticket will be paid on time.\n",
    "\n",
    "All data for this assignment has been provided to us through the [Detroit Open Data Portal](https://data.detroitmi.gov/). **Only the data already included in your Coursera directory can be used for training the model for this assignment.** Nonetheless, we encourage you to look into data from other Detroit datasets to help inform feature creation and model selection. We recommend taking a look at the following related datasets:\n",
    "\n",
    "* [Building Permits](https://data.detroitmi.gov/Property-Parcels/Building-Permits/xw2a-a7tf)\n",
    "* [Trades Permits](https://data.detroitmi.gov/Property-Parcels/Trades-Permits/635b-dsgv)\n",
    "* [Improve Detroit: Submitted Issues](https://data.detroitmi.gov/Government/Improve-Detroit-Submitted-Issues/fwz3-w3yn)\n",
    "* [DPD: Citizen Complaints](https://data.detroitmi.gov/Public-Safety/DPD-Citizen-Complaints-2016/kahe-efs3)\n",
    "* [Parcel Map](https://data.detroitmi.gov/Property-Parcels/Parcel-Map/fxkw-udwf)\n",
    "\n",
    "___\n",
    "\n",
    "We provide you with two data files for use in training and validating your models: train.csv and test.csv. Each row in these two files corresponds to a single blight ticket, and includes information about when, why, and to whom each ticket was issued. The target variable is compliance, which is True if the ticket was paid early, on time, or within one month of the hearing data, False if the ticket was paid after the hearing date or not at all, and Null if the violator was found not responsible. Compliance, as well as a handful of other variables that will not be available at test-time, are only included in train.csv.\n",
    "\n",
    "Note: All tickets where the violators were found not responsible are not considered during evaluation. They are included in the training set as an additional source of data for visualization, and to enable unsupervised and semi-supervised approaches. However, they are not included in the test set.\n",
    "\n",
    "<br>\n",
    "\n",
    "**File descriptions** (Use only this data for training your model!)\n",
    "\n",
    "    train.csv - the training set (all tickets issued 2004-2011)\n",
    "    test.csv - the test set (all tickets issued 2012-2016)\n",
    "    addresses.csv & latlons.csv - mapping from ticket id to addresses, and from addresses to lat/lon coordinates. \n",
    "     Note: misspelled addresses may be incorrectly geolocated.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Data fields**\n",
    "\n",
    "train.csv & test.csv\n",
    "\n",
    "    ticket_id - unique identifier for tickets\n",
    "    agency_name - Agency that issued the ticket\n",
    "    inspector_name - Name of inspector that issued the ticket\n",
    "    violator_name - Name of the person/organization that the ticket was issued to\n",
    "    violation_street_number, violation_street_name, violation_zip_code - Address where the violation occurred\n",
    "    mailing_address_str_number, mailing_address_str_name, city, state, zip_code, non_us_str_code, country - Mailing address of the violator\n",
    "    ticket_issued_date - Date and time the ticket was issued\n",
    "    hearing_date - Date and time the violator's hearing was scheduled\n",
    "    violation_code, violation_description - Type of violation\n",
    "    disposition - Judgment and judgement type\n",
    "    fine_amount - Violation fine amount, excluding fees\n",
    "    admin_fee - $20 fee assigned to responsible judgments\n",
    "state_fee - $10 fee assigned to responsible judgments\n",
    "    late_fee - 10% fee assigned to responsible judgments\n",
    "    discount_amount - discount applied, if any\n",
    "    clean_up_cost - DPW clean-up or graffiti removal cost\n",
    "    judgment_amount - Sum of all fines and fees\n",
    "    grafitti_status - Flag for graffiti violations\n",
    "    \n",
    "train.csv only\n",
    "\n",
    "    payment_amount - Amount paid, if any\n",
    "    payment_date - Date payment was made, if it was received\n",
    "    payment_status - Current payment status as of Feb 1 2017\n",
    "    balance_due - Fines and fees still owed\n",
    "    collection_status - Flag for payments in collections\n",
    "    compliance [target variable for prediction] \n",
    "     Null = Not responsible\n",
    "     0 = Responsible, non-compliant\n",
    "     1 = Responsible, compliant\n",
    "    compliance_detail - More information on why each ticket was marked compliant or non-compliant\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Your predictions will be given as the probability that the corresponding blight ticket will be paid on time.\n",
    "\n",
    "The evaluation metric for this assignment is the Area Under the ROC Curve (AUC). \n",
    "\n",
    "Your grade will be based on the AUC score computed for your classifier. A model which with an AUROC of 0.7 passes this assignment, over 0.75 will recieve full points.\n",
    "___\n",
    "\n",
    "For this assignment, create a function that trains a model to predict blight ticket compliance in Detroit using `train.csv`. Using this model, return a series of length 61001 with the data being the probability that each corresponding ticket from `test.csv` will be paid, and the index being the ticket_id.\n",
    "\n",
    "Example:\n",
    "\n",
    "    ticket_id\n",
    "       284932    0.531842\n",
    "       285362    0.401958\n",
    "       285361    0.105928\n",
    "       285338    0.018572\n",
    "                 ...\n",
    "       376499    0.208567\n",
    "       376500    0.818759\n",
    "       369851    0.018528\n",
    "       Name: compliance, dtype: float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# python standard library\n",
    "import re\n",
    "\n",
    "# pypi\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "CATEGORICAL = \"O\"\n",
    "TARGET = \"compliance\"\n",
    "COLUMNS = {\"ticket_id\",\n",
    "           \"agency_name\",\n",
    "           \"inspector_name\",\n",
    "           \"violator_name\",\n",
    "           \"violation_street_number\", \"violation_street_name\", \"violation_zip_code\",\n",
    "           \"mailing_address_str_number\", \"mailing_address_str_name\", \"city\", \"state\", \"zip_code\", \"non_us_str_code\", \"country\",\n",
    "           \"ticket_issued_date\",\n",
    "           \"hearing_date\",\n",
    "           \"violation_code\", \"violation_description\",\n",
    "           \"disposition\",\n",
    "           \"fine_amount\",\n",
    "           \"admin_fee\",\n",
    "           \"state_fee\",\n",
    "           \"late_fee\",\n",
    "           \"discount_amount\",\n",
    "           \"clean_up_cost\",\n",
    "           \"judgment_amount\",\n",
    "           \"grafitti_status\",}\n",
    "keep_columns = COLUMNS.copy()\n",
    "dummy_columns = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class DataSources(object):\n",
    "    training_file = \"train.csv\"\n",
    "    testing_file = \"test.csv\"\n",
    "    addresses_file = \"addresses.csv\"\n",
    "    latitude_longitude_file = \"latlons.csv\"\n",
    "    encoding=\"latin1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class CleanZips(object):\n",
    "    \"\"\"a class to hold methods to clean zip codes\"\"\"\n",
    "    @classmethod\n",
    "    def non_digit(cls, row):\n",
    "        \"\"\"checks to see if a string can be converted to an integer\n",
    "\n",
    "        Args:\n",
    "         row: integer or string\n",
    "\n",
    "        Returns:\n",
    "         bool: True if row is a string that cannot be converted to an int\n",
    "        \"\"\"\n",
    "        return type(row) is str and not row.isdigit()\n",
    "\n",
    "    @classmethod\n",
    "    def non_digits(cls, source):\n",
    "        \"\"\"gets all values that can't be integers\n",
    "\n",
    "        Args:\n",
    "         Series: data with zip-codes to check\n",
    "        Returns:\n",
    "         Series: zip-codes that can't be integers\n",
    "        \"\"\"\n",
    "        return source.zip_code[source.zip_code.apply(cls.non_digit)]\n",
    "\n",
    "    @classmethod\n",
    "    def strip_suffix(cls, row):\n",
    "        \"\"\"strips anything following a '-' (including the dash)\n",
    "        \n",
    "        Args:\n",
    "         row: something that might have a suffix to strip_suffix\n",
    "        Returns:\n",
    "         int or str: row without '-'\n",
    "        \"\"\"\n",
    "        if type(row) is str and \"-\" in row:\n",
    "            row = row.split(\"-\")[0]\n",
    "        return row\n",
    "\n",
    "    @classmethod\n",
    "    def convert_to_int(cls, row):\n",
    "        \"\"\"tries to convert entry to integer\n",
    "\n",
    "        Args:\n",
    "         row (str | int): thing to convert\n",
    "        \n",
    "        Returns:\n",
    "         int | str: row as integer or original string if it can't be converted\n",
    "        \"\"\"\n",
    "        try:\n",
    "            row = int(row)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        return row\n",
    "\n",
    "    @classmethod\n",
    "    def strip_and_convert(cls, source):\n",
    "        \"\"\"strips '-' suffixes from strings and converts them to integers\n",
    "\n",
    "        Args:\n",
    "         source (DataFrame): data with 'zip_code' column\n",
    "\n",
    "        Returns:\n",
    "         DataFrame: data with whatever zip codes could be converted to integers\n",
    "        \"\"\"\n",
    "        source[\"zip_code\"] = source.zip_code.apply(cls.strip_suffix)\n",
    "        source[\"zip_code\"] = source.zip_code.apply(cls.convert_to_int)\n",
    "        return source\n",
    "\n",
    "    @classmethod\n",
    "    def to_zero(cls, row):\n",
    "        \"\"\"converts any strings to 0\"\"\"\n",
    "        if type(row) is str:\n",
    "            row = 0\n",
    "        return row\n",
    "\n",
    "    @classmethod\n",
    "    def apply_zeros(cls, source):\n",
    "        \"\"\"converts all zip codes that are strings to zero\"\"\"\n",
    "        source[\"zip_code\"] = source.zip_code.apply(cls.to_zero)\n",
    "        return source\n",
    "\n",
    "    @classmethod\n",
    "    def impute_missing(cls, source):\n",
    "        \"\"\"sets NaN zip_codes to the most common zip code\"\"\"\n",
    "        most_common = source.zip_code.value_counts().index[0]\n",
    "        source.zip_code.fillna(most_common, inplace=True)\n",
    "        return source\n",
    "\n",
    "    @classmethod\n",
    "    def clean(cls, source):\n",
    "        \"\"\"cleans the zip-codes in the source\"\"\"\n",
    "        source = cls.impute_missing(source)\n",
    "        source = cls.strip_and_convert(source)\n",
    "        source = cls.apply_zeros(source)\n",
    "        return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class CleanTimestamps(object):\n",
    "    \"\"\"converts timestamp columns to unix epoch-times\n",
    "    \"\"\"\n",
    "    columns = (\"ticket_issued_date\",)\n",
    "\n",
    "    @classmethod\n",
    "    def to_timestamp(cls, column, source):\n",
    "        \"\"\"convert the column to a unix epoch timestamp\n",
    "        \n",
    "        Args:\n",
    "         column (str): name of the column to convert\n",
    "         source (DataFrame): data with the column to convert\n",
    "        \n",
    "        Returns:\n",
    "         DataFrame: the converted data frame\n",
    "        \"\"\"\n",
    "        source[column] = pandas.to_datetime(source[column])\n",
    "        source[column] = source[column].apply(lambda row: int(row.timestamp()))\n",
    "        return source\n",
    "\n",
    "    @classmethod\n",
    "    def clean(cls, source):\n",
    "        \"\"\"converts the source's columns to timestamps\n",
    "\n",
    "        Args:\n",
    "         source (DataFrame): data with date columns\n",
    "        \"\"\"\n",
    "        for column in cls.columns:\n",
    "            source = cls.to_timestamp(column, source)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class AddLatitudeLongitude(object):\n",
    "    \"\"\"adds the latitude and longitude\n",
    "\n",
    "    Args:\n",
    "     source (object): holder of the address and lat/lon data source names\n",
    "     common_column (str): name of column to use when merging with other data\n",
    "    \"\"\"\n",
    "    zip_pattern = re.compile(r\"\\s\\d+$\")\n",
    "    # note that some zero's are interpreted as capital O's for some reason\n",
    "    na_streets = (\"12038 prairie, Detroit MI 482O4\",\n",
    "                  \"20424 bramford, Detroit MI\",\n",
    "                  \"6200 16th st, Detroit MI 482O8\",\n",
    "                  \"8325 joy rd, Detroit MI 482O4\",\n",
    "                  \"1201 elijah mccoy dr, Detroit MI 48208\",\n",
    "                  \"8300 fordyce, Detroit MI\",\n",
    "                  \"445 fordyce, Detroit MI\")\n",
    "    na_lats = (42.37613, 42.446574, 42.359923, 42.358858, 42.35853, 42.383262, 4233998)\n",
    "    na_lons = (-83.14312, -83.023178, -83.095686, -83.151228, -83.080371, -83.058238, -83.05855)\n",
    "    lat_map = dict(zip(na_streets, na_lats))\n",
    "    lons_map = dict(zip(na_streets, na_lons))\n",
    "\n",
    "    def __init__(self, source=DataSources, common_column=\"ticket_id\"):\n",
    "        self.source = source\n",
    "        self.common_column = common_column\n",
    "        self._addresses = None\n",
    "        self._latitude_longitude = None\n",
    "        self._merged = None\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def addresses(self):\n",
    "        \"\"\"the address data\n",
    "\n",
    "        Returns:\n",
    "         DataFrame: data mapping ticket_id to address\n",
    "        \"\"\"\n",
    "        if self._addresses is None:\n",
    "            self._addresses = pandas.read_csv(self.source.addresses_file)\n",
    "            self._addresses = self.strip_zip(self._addresses)\n",
    "            assert len(self.zip_rows(self._addresses)) == 0\n",
    "        return self._addresses\n",
    "\n",
    "    @property\n",
    "    def latitude_longitude(self):\n",
    "        \"\"\"the latitude and longitude data\n",
    "        \n",
    "        Returns:\n",
    "         DataFrame: data mapping address to latitude and longitude\n",
    "        \"\"\"\n",
    "        if self._latitude_longitude is None:\n",
    "            self._latitude_longitude = pandas.read_csv(\n",
    "                self.source.latitude_longitude_file)\n",
    "            self._latitude_longitude = self.fill_nas(self._latitude_longitude)\n",
    "            assert not self._latitude_longitude.lat.hasnans\n",
    "            assert not self._latitude_longitude.lon.hasnans\n",
    "            self._latitude_longitude = self.strip_zip(self._latitude_longitude)\n",
    "            assert len(self.zip_rows(self._latitude_longitude)) == 0\n",
    "            self._latitude_longitude = (self._latitude_longitude\n",
    "                                        .drop_duplicates(subset=[\"address\"]))\n",
    "        return self._latitude_longitude\n",
    "\n",
    "    @property\n",
    "    def merged(self):\n",
    "        \"\"\"addresses and latitude/longitude merged\n",
    "\n",
    "        Returns:\n",
    "         DataFrame: data with ticket_id, address, lat, and lon\n",
    "        \"\"\"\n",
    "        if self._merged is None:\n",
    "            self._merged = pandas.merge(self.addresses, self.latitude_longitude,\n",
    "                                        on=\"address\")\n",
    "            assert sum(self._merged.duplicated(subset=\"ticket_id\")) == 0\n",
    "        return self._merged\n",
    "\n",
    "    def fill_nas(self, source):\n",
    "        \"\"\"fills in the known missing data\n",
    "        \n",
    "        Args:\n",
    "         source (DataFrame): the latitude-longitude data\n",
    "        \"\"\"\n",
    "        def replace(row):\n",
    "            if row.address in self.lat_map:\n",
    "                row[\"lat\"] = self.lat_map[row.address]\n",
    "            if row.address in self.lons_map:\n",
    "                row[\"lon\"] = self.lons_map[row.address]\n",
    "            return row\n",
    "        source = source.apply(replace, axis=1)\n",
    "        return source\n",
    "\n",
    "    def has_zip(self, row):\n",
    "        \"\"\"checks if the row has a zip code\n",
    "\n",
    "        Args:\n",
    "         row (str): entry to check\n",
    "        Returns:\n",
    "         bool: True if there are digits at the end of the row\n",
    "        \"\"\"\n",
    "        return self.zip_pattern.search(row) is not None\n",
    "\n",
    "    def zip_rows(self, source):\n",
    "        \"\"\"gets the rows with zip codes\n",
    "\n",
    "        Args:\n",
    "         source (DataFrame): data with 'address' column\n",
    "\n",
    "        Returns:\n",
    "         DataFrame: rows from the source that have a zip code\n",
    "        \"\"\"\n",
    "        return source[source.address.apply(self.has_zip)]\n",
    "\n",
    "    def strip_zip(self, source):\n",
    "        \"\"\"removes zip-codes from addresses\n",
    "\n",
    "        Args:\n",
    "         source (DataFrame): data with 'address' column to process\n",
    "        \"\"\"\n",
    "        source[\"address\"] = source.address.replace(self.zip_pattern, \"\",\n",
    "                                                   regex=True)\n",
    "        return source\n",
    "\n",
    "    def __call__(self, source):\n",
    "        \"\"\"adds latitude and longitude to the data\n",
    "\n",
    "        Args:\n",
    "         source (DataFrame): data with ticket_id to match to latitude/longitude\n",
    "\n",
    "        Returns:\n",
    "         DataFrame: source with added latitude and longitude\n",
    "        \"\"\"\n",
    "        source = pandas.merge(source, self.merged, on=self.common_column)\n",
    "        return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class AddDummies(object):\n",
    "    \"\"\"class to add dummy columns\n",
    "\n",
    "    Args:\n",
    "     training (DataFrame): data ready to add dummies to\n",
    "     testing (DataFrame): data with same columns as testing\n",
    "    \"\"\"\n",
    "    columns = [\"agency_name\", \"inspector_name\", \"country\", \"violation_code\",\n",
    "               \"disposition\"]\n",
    "    def __init__(self, training, testing):\n",
    "        self.training = training\n",
    "        self.testing = testing\n",
    "        self._testing_start = None\n",
    "        self._concatenated = None\n",
    "        self._with_dummies = None\n",
    "        self._training_with_dummies = None\n",
    "        self._testing_with_dummies = None\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def testing_start(self):\n",
    "        \"\"\"the index of the row in concatenated data where testing starts\n",
    "\n",
    "        Returns:\n",
    "         int: index to start testing data in concatenated data\n",
    "        \"\"\"\n",
    "        if self._testing_start is None:\n",
    "            self._testing_start = len(self.training)\n",
    "        return self._testing_start\n",
    "\n",
    "    @property\n",
    "    def concatenated(self):\n",
    "        \"\"\"concatenated training and testing set\n",
    "        \n",
    "        Returns:\n",
    "         DataFrame: testing concatenated to end of training\n",
    "        \"\"\"\n",
    "        if self._concatenated is None:\n",
    "            self._concatenated = pandas.concat(objs=[self.training,\n",
    "                                                     self.testing], axis=0)\n",
    "        return self._concatenated\n",
    "\n",
    "    @property\n",
    "    def with_dummies(self):\n",
    "        \"\"\"concatenated data with dummy variables added\n",
    "\n",
    "        Returns:\n",
    "         DataFrame: concatenaded data with dummy variables\n",
    "        \"\"\"\n",
    "        if self._with_dummies is None:\n",
    "            self._with_dummies = pandas.get_dummies(self.concatenated,\n",
    "                                                    columns=self.columns)\n",
    "        return self._with_dummies\n",
    "\n",
    "    @property\n",
    "    def training_with_dummies(self):\n",
    "        \"\"\"the training data with dummy variables\"\"\"\n",
    "        if self._training_with_dummies is None:\n",
    "            self._training_with_dummies = self.with_dummies[:self.testing_start]\n",
    "            assert len(self._training_with_dummies) == len(self.training)\n",
    "        return self._training_with_dummies\n",
    "\n",
    "    @property\n",
    "    def testing_with_dummies(self):\n",
    "        \"\"\"the testing data with dummy variables\"\"\"\n",
    "        if self._testing_with_dummies is None:\n",
    "            self._testing_with_dummies = self.with_dummies[self.testing_start:]\n",
    "            assert len(self._testing_with_dummies) == len(self.testing)\n",
    "        return self._testing_with_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class CleanedData(object):\n",
    "    \"\"\"loads and cleans the data sets\n",
    "\n",
    "    Args:\n",
    "     columns (set): columns common to both training and testing\n",
    "     target_column (str): the prediction target\n",
    "     sources (object): class with the strings to load the data\n",
    "     index_column (str): column to use as the index of the data-frames\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=COLUMNS, target_column=\"compliance\",\n",
    "                 sources=DataSources, index_column=\"ticket_id\"):\n",
    "        self.columns = columns\n",
    "        self.target_column = target_column\n",
    "        self.sources = DataSources\n",
    "        self.index_column = index_column\n",
    "        self._training = None\n",
    "        self._testing = None\n",
    "        self._keep_columns = None\n",
    "        self._drop_nas = None\n",
    "        self._add_latitude_longitude = None\n",
    "        self._labels = None\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def add_latitude_longitude(self):\n",
    "        \"\"\"callable to add latitude and longitude to data\"\"\"\n",
    "        if self._add_latitude_longitude is None:\n",
    "            self._add_latitude_longitude = AddLatitudeLongitude()\n",
    "        return self._add_latitude_longitude\n",
    "\n",
    "    @property\n",
    "    def keep_columns(self):\n",
    "        \"\"\"set of columns to keep (without target added)\"\"\"\n",
    "        if self._keep_columns is None:\n",
    "            self._keep_columns = self.columns.copy()\n",
    "            for column in (\"violator_name\", \"violation_street_number\",\n",
    "                           \"violation_street_name\", \"violation_zip_code\",\n",
    "                           \"mailing_address_str_number\", \"hearing_date\",\n",
    "                           \"mailing_address_str_name\", \"city\", \"state\",\n",
    "                           \"non_us_str_code\", \"grafitti_status\",\n",
    "                           \"violation_description\"):\n",
    "                self._keep_columns.remove(column)\n",
    "            self._keep_columns.add(\"lat\")\n",
    "            self._keep_columns.add(\"lon\")\n",
    "        return self._keep_columns\n",
    "\n",
    "    @property\n",
    "    def drop_nas(self):\n",
    "        \"\"\"list of columns to drop NaNs from\"\"\"\n",
    "        if self._drop_nas is None:\n",
    "            self._drop_nas = [\"compliance\"]\n",
    "        return self._drop_nas\n",
    "                \n",
    "\n",
    "    @property\n",
    "    def training(self):\n",
    "        \"\"\"The original training set\n",
    "\n",
    "        This will only include the columns common to both traning and testing\n",
    "        sets plus the target\n",
    "\n",
    "        Returns:\n",
    "         DataFrame: the given training data set\n",
    "        \"\"\"\n",
    "        if self._training is None:\n",
    "            self._training = pandas.read_csv(DataSources.training_file,\n",
    "                                             encoding=DataSources.encoding)\n",
    "            columns = self.columns.copy()\n",
    "            columns.add(self.target_column)\n",
    "            self._training = self._training[list(columns)]\n",
    "            # I was removing some rows to deal with NaNs, but the \n",
    "            # final answer has to have all the rows so this won't work\n",
    "            # other than to get rid of the extra compliance rows\n",
    "            self._training.dropna(subset=self.drop_nas, inplace=True)\n",
    "            self._training = self.clean_source(self._training)\n",
    "            keep_columns = self.keep_columns.copy()\n",
    "            keep_columns.add(self.target_column)\n",
    "            self._training = self._training[list(keep_columns)]\n",
    "            # we need to keep the labels with the training data while dropping\n",
    "            # rows, but it will mess it up if we don't split them up before\n",
    "            # using the data\n",
    "            self._labels = self._training[self.target_column]\n",
    "            del self._training[self.target_column]\n",
    "            self._training.set_index(self.index_column, inplace=True)\n",
    "        return self._training\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        \"\"\"the labels for the training data\n",
    "        \n",
    "        Warning:\n",
    "         as a side effect this will delete the target column from the training data\n",
    "        \"\"\"\n",
    "        if self._labels is None:\n",
    "            self._labels = self.training[self.target_column]\n",
    "            del self._training[self.target_column]\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def testing(self):\n",
    "        \"\"\"the testing set cleaned up\n",
    "\n",
    "        Returns:\n",
    "         DataFrame: the given testing set\n",
    "        \"\"\"\n",
    "        if self._testing is None:\n",
    "            self._testing = pandas.read_csv(DataSources.testing_file)\n",
    "            self._testing = self.clean_source(self._testing)\n",
    "            self._testing = self._testing[list(self.keep_columns)]\n",
    "            self._testing.set_index(self.index_column, inplace=True)\n",
    "        return self._testing\n",
    "\n",
    "    def clean_source(self, source):\n",
    "        \"\"\"does the cleaning for the given data-frame\n",
    "        \n",
    "        Warning:\n",
    "         Because there's a merge done, you have to use the returned frame\n",
    "\n",
    "        Returns:\n",
    "         DataFrame: transformed data\n",
    "        \"\"\"\n",
    "        start = len(source)\n",
    "        CleanZips.clean(source)\n",
    "        assert len(CleanZips.non_digits(source)) == 0\n",
    "        CleanTimestamps.clean(source)\n",
    "        source = self.add_latitude_longitude(source)\n",
    "        assert len(source) == start\n",
    "        return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def blight_model():\n",
    "    cleaned = CleanedData()\n",
    "    dummies = AddDummies(cleaned.training, cleaned.testing)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        dummies.training_with_dummies,\n",
    "        cleaned.labels)\n",
    "    forest_base = RandomForestClassifier(n_estimators=19, max_features=22)\n",
    "    forest_base.fit(x_train, y_train)\n",
    "    probabilities = forest_base.predict_proba(dummies.testing_with_dummies)\n",
    "    solution = pandas.DataFrame(probabilities[:,1], index=dummies.testing_with_dummies.index)\n",
    "    return solution"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "nNS8l",
   "launcher_item_id": "yWWk7",
   "part_id": "w8BSS"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "assignment_4_submission.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
